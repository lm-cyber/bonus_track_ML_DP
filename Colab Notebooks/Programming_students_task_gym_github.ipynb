{"cells":[{"cell_type":"markdown","metadata":{"id":"q9QLe_T6GZUd"},"source":["# Задание на программирование"]},{"cell_type":"markdown","metadata":{"id":"EYlIf2yHv8hz"},"source":["**Выполнять задание следует с текущими значениями гиперпараметров. Для проверки ниже будут приведены ответы, которые должны получиться в результате выполнения задания. После того, как все ответы совпадут, можно будет использовать полученный блокнот для выполнения индивидуального задания.**"]},{"cell_type":"markdown","metadata":{"id":"ZDQzNIZXAoFE"},"source":["Зададим гиперпараметры модели"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NOMw2ZbOAmOZ"},"outputs":[],"source":["epsilon = 0.1 # Параметр эпсилон при использовании эпсилон жадной стратегии\n","gamma = 0.9 # Коэффциент дисконтирования гамма\n","random_seed = 6 #Random seed\n","time_delay = 1 # Задержка времени при отрисовке процесса игры после обучения (секунды)\n","lr_rate = 0.9 #Коэффициент скорости обучения альфа"]},{"cell_type":"markdown","metadata":{"id":"pQu5IYHX8jId"},"source":["Импортируем библиотеки, создаем свою среду размера 6х6. S обозначает точку старта. F -- лед безопасен, H -- проталина, G -- цель. Параметр `is_slippery=False` отвечает за условное отсутствие скольжения. То есть если агент выбрал действие пойти направо, то он переместится в соответствующее состояние. В общем случае из-за \"скольжения\" можно оказаться в другом состоянии. Мы также скопировали из библиотки GYM и слегка модифицировали функцию ```generate_random_map ```, для того, чтобы генерировать произвольные карты на основе ```random_seed ```.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2G81i4_lOQE"},"outputs":[],"source":["# Установим нужную версию библиотеки gym\n","!git clone https://github.com/dvolchek/gym_0_18_0.git -q\n","%cd /content/gym_0_18_0\n","!pip install -e. -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153},"id":"awL7CCCwD6C3","outputId":"5b2d42db-dc19-4cef-f753-805b8b6be9c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ваша карта\n","\n","\u001b[41mS\u001b[0mFFHFF\n","FHFFHF\n","FFFHHF\n","HFFHHF\n","FFFFFF\n","FFFFFG\n"]}],"source":["import gym\n","import numpy as np\n","import time\n","from IPython.display import clear_output\n","\n","\n","def generate_random_map(size, p, sd):\n","    \"\"\"Generates a random valid map (one that has a path from start to goal)\n","    :param size: size of each side of the grid\n","    :param p: probability that a tile is frozen\n","    \"\"\"\n","    valid = False\n","    np.random.seed(sd)\n","\n","    # DFS to check that it's a valid path.\n","    def is_valid(res):\n","        frontier, discovered = [], set()\n","        frontier.append((0,0))\n","        while frontier:\n","            r, c = frontier.pop()\n","            if not (r,c) in discovered:\n","                discovered.add((r,c))\n","                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","                for x, y in directions:\n","                    r_new = r + x\n","                    c_new = c + y\n","                    if r_new \u003c 0 or r_new \u003e= size or c_new \u003c 0 or c_new \u003e= size:\n","                        continue\n","                    if res[r_new][c_new] == 'G':\n","                        return True\n","                    if (res[r_new][c_new] not in '#H'):\n","                        frontier.append((r_new, c_new))\n","        return False\n","\n","    while not valid:\n","        p = min(1, p)\n","        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n","        res[0][0] = 'S'\n","        res[-1][-1] = 'G'\n","        valid = is_valid(res)\n","    return [\"\".join(x) for x in res]\n","\n","#Генерация карты\n","random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Создаем свою карту\n","env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Инициализируем среду\n","print(\"Ваша карта\")\n","env.render() #Выводим карту на экран"]},{"cell_type":"markdown","metadata":{"id":"MDCexoEU9a_c"},"source":["Функции выбора действия и обновления таблицы ценности действий. Строчка *** используется для того, чтобы проверять ответы в openedx. Вне рамках академической задачи лучше использовать оригинальный метод класса `environment`, то есть:\n","\n","`action = env.action_space.sample()`"]},{"cell_type":"markdown","metadata":{"id":"D5TbDqn6G_Pt"},"source":["# Задача 1\n","Дополните функцию ```learn()```, чтобы в результате ее вызова обновлялось значение ценности текущего действия согласно алгоритму Q-обучения\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdQBpxaTOK7u"},"outputs":[],"source":["def choose_action(state):\n","    action=0\n","    if np.random.uniform(0, 1) \u003c epsilon:\n","        action = np.random.randint(0,env.action_space.n) #***\n","    else:\n","        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n","    return action\n","\n","def learn(state, state2, reward, action, done):\n","    #Q[state, action] = #Ваш код здесь"]},{"cell_type":"markdown","metadata":{"id":"7COGeyA_Ist3"},"source":["# Задача 2\n","Дополните следующий код так, чтобы в результате обучения модели можно было узнать количество побед и номер игры (`game`), на котором агент впервые одержал пятую победу подряд."]},{"cell_type":"markdown","metadata":{"id":"0adDl7NvJoQP"},"source":["Поясним, что возвращает функция ```env.step(action)```\n","\n","```state2``` -- следующее состояние\n","\n","```reward``` -- награда\n","\n","```done``` -- флаг окончания игры. True в случае победы или падения в проталину. False в остальных случаях.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"aq92-dWiOchF","outputId":"91ec4dc4-fb39-4818-ac78-79c9fe6d0ee7"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [00:37\u003c00:00, 267.20it/s]\n"]}],"source":["from tqdm import tqdm\n","# Inititalization\n","np.random.seed(random_seed)\n","total_games = 10000\n","max_steps = 100\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","#Main cycle\n","for game in tqdm(range(total_games)):\n","    state = env.reset()\n","    t = 0\n","    while t \u003c max_steps:\n","        \n","        t += 1\n","\n","        action = choose_action(state)\n","\n","        state2, reward, done, info = env.step(action)\n","\n","        if t == max_steps:\n","          done = True  \n","\n","        learn(state, state2, reward, action, done)\n","\n","        state = state2\n","\n","        if done:\n","          break\n"]},{"cell_type":"markdown","metadata":{"id":"JFuxsqdRLOS9"},"source":["Вывод ответов при заданных параметрах"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZbJtFnhLa7w"},"outputs":[],"source":["print(\"Количество побед в серии из 10 000 игр: \", #ваш код здесь)\n","print(\"Пять побед подряд впервые было одержано в игре \", #ваш код здесь)\n"]},{"cell_type":"markdown","metadata":{"id":"TSXdSiG2WI71"},"source":["Должны получиться следующие результаты.\n","\n","\n","*  Количество побед в серии из 10 000 игр:  7914\n","*  Пять побед подряд впервые было одержано в игре  885\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nazZaAbwQGBt"},"source":["Произведем одну игру, чтобы проследить за действиями агента. При этом будем считать модель полностью обученной, то есть действия выбираются жадно, значения ценностей действий в таблице не обновляются."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"5ysllZjEQXLa","outputId":"29ec2e79-a0d5-4fcb-a551-6209d40dd7ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["!!!Победа!!!\n"]}],"source":["import time\n","#Жадный выбор действий\n","def choose_action_one_game(state):\n","    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n","    return action\n","\n","states=[]#Массив для сохранения состояний агента в течение игры\n","t = 0\n","state = env.reset()\n","wn = 0\n","while(t\u003c100):\n","  env.render()\n","  time.sleep(time_delay)\n","  clear_output(wait=True)\n","  action = choose_action_one_game(state)  \n","  state2, reward, done, info = env.step(action)  \n","  states.append(state)\n","  state = state2\n","  t += 1\n","  if done and reward == 1:\n","    wn=1\n","  if done:\n","    break\n","if wn == 1:\n","  print(\"!!!Победа!!!\")"]},{"cell_type":"markdown","metadata":{"id":"x696NulpReFI"},"source":["Отобразим маршрут"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"UKMCMdpOTcXy","outputId":"bd9a32aa-b615-407f-bb4b-9a2ae654df4f"},"outputs":[{"data":{"text/plain":["\u003cmatplotlib.image.AxesImage at 0x7f6e17a6e160\u003e"]},"execution_count":8,"metadata":{"tags":[]},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQIUlEQVR4nO3db2wc9Z3H8c/kD0ZrhzgCslFj7IWqMa1y1HebtuEKsq17cAURXf88KMmKoyFl+0enk5sCvWJxgUquejqfzg9AQmtV8ICtrbSI5i6o6tHidSnSVrWvFhBdwvUu2LghLuQw2N7GJPbvHgzGbLxO1sl8PTub90sa2fOb8W8+XnY/zIzXjuecEwBYWBN2AADVi4IBYIaCAWCGggFghoIBYGbdSna+5pprXCKRMIoSvJmZGdXW1oYdoyxRyipJJ06c0BtvvBF2jLLdeOONkXp8o/Z8GB4efss5d+2SDc65spdkMumiZGBgIOwIZYtSVuec6+7udpIis0Tt8Y1aXklDrkRnrOgMZsGW7i2amJm4mC9dFfHauE7edzLsGMBl76LuwVRyuUiVnw+4XHCTF4AZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgBkKBoAZCgaAGQoGgJlwC+al3dK/HpcenvM/vrQ71DgAgnVRf9EuEC/tlv69Vzrz/t8dfSfhr0vSTX2hxQIQnPDOYH75/cVyWXCm1h8HUBXCK5h3Glc2DiBywiuYjWMrGwcQOeEVzF89KK2fKR5bP+OPA6gK4RXMTX3SrnultaclOWnja/46N3iBqhF4wXz2us/qxXte1OR3JnXqgVP69d5fa8dHdujuT96tF/a+ULzzTX1SQ15qGpS+df0Fy6VpY5PcAae13tqgYwMwEOiPqTdcsUGH9xzWN579hg4eOagr1l6hWxtv1ezZ2Uuem1IBoifQM5htV2+TJPW/0q95N6/TZ0/ruf99Tmfmz+jxOx7XzQ03a+q7U3r7O29Lkm7/2O36z2eTeuflWzTWMaYDrQc+mGvhbOWeP79Hox2jev7u5/Wrvb+SJE3+w6SmvjulnQ07g4wPIGCBnsG8eupVzc3P6cm/eVL9R/qVH89r8vSkjr51VF8//HV99S++qlufuPWD/Wfem9Hf7j+qI6/OaPsDf6/n7npOIydHdOjYoQ/2aW1q1ccf+7jm3bzitXG91vGa6n9Qrzk3F2R0AAYCPYOZem9Ktzxxi5ycenf16s3739ShOw9pc+3mkvsPjg7qlWMzck56+Y8vq++VPrUmWov2eTj3sApnCjp99nSQUQGsgsB/VeDoW0e199BeSVLz1c166otPqeeve/Tz//n5kn0/vfXT+kHfJ7V9W62uqJ1Uzboa/fjIj4v2ef3d14OOCGCVmP6Y+tipY3py5Elt37xdTm7J9h998Uf6t1+c0nV/mVf9P9Xr8aHH5Xle0T7OLX5dqTkAVK5AC6b56mbtv3m/tm7YKklquKpBu7fvVv4PeU1MT6jhqgatX7P+g/031GzQ/02e0ezsvD71kU9pz5/tOe/8b868qbn5Od2w6YYgYwMwEugl0tR7U/rM1s9o/879qr+yXpOnJ3X4vw/r/v+4X6fPntaRPx7RyftOat7N69p/vlbffPab+pdvPaVHH/mYBk/8ow4eOaj6K+uXnf9PZ/+krhe69OI9L2r92vX63FOf02/+8JsgvwUAAQq0YE5MndCXf/LlZbff0XdH0frT//W0nn7g7/yVvbuKto2+MyrvkeLLJUk6kDugA7kDS8YBVB7+oh0AMxQMADMUDAAzFAwAMxQMADMUDAAzF1Uw8dp40DkCVen5gMvFRb0P5uR9JwML0Dbgf8wd4NcAgGrjffh3fUru4HlpSWlJisfjyf7+/kADdHS0SJJ6ekYCnVeSJiYmND4+Hvi8FhoaGhSPR+fMa3p6WnV1dWHHKFuUnguS1NzcHKnHt729fdg5t2PJBudc2UsymXRBa231Fwvd3d1OUiSW7u5umwfByMDAQNgRViRKzwVJkXt8JQ25Ep3BTV4AZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGZCLZhsVsrnpcFBKZHw1wFUj9AKJpuV0mlpdtZfHx311ykZoHqEVjCdnVKhUDxWKPjjAKpDaAUzNraycQDRE1rBNDaubBxA9IRWMF1dUixWPBaL+eMAqkNoBZNKSZmMVFPjrzc1+eupVFiJAATtov7p2KCkUlJvr/95LhdmEgAWeKMdADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAznnPu/Dt4XlpSWpLi8Xiyv78/0AAdHS2SpJ6ekUDnlaTp6WnV1dUFPq+FKGWVpImJCY2Pj4cdo2wNDQ2Rytvc3Byp50N7e/uwc27Hkg3OubKXZDLpgtba6i8WBgYGbCY2EKWszjnX3d3tJEVmiVreqD0fJA25Ep3BJRIAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADOhFkw2K+Xz0uCglEj465Uqm/UzrllT+VmBSrEurANns1I6Lc3O+uujo/66JKVSYaUqbSFroeCvV3JWoJKEVjCdnYsv2AWFgrRvn9TbG8wxJidbVF9/6fPk84tFuKBQ8L8HCgZYXmiXSGNjpcfPfSFXguUyLfc9APCFdgbT2OhfapyrqUnK5YI5Ri43ora2tkueJ5EonbWx8ZKnBqpaaGcwXV1SLFY8Fov545UmSlmBShJawaRSUibjn7F4nv8xk6nMexoLWWtq/PVKzgpUktAukST/BRqVF2kqtXjzOahLOKDa8UY7AGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGYoGABmKBgAZigYAGZWVDDDw8PyPC8yC4Bwec658+/geWlJaUnauHFj8qGHHlqNXIFobm5WXV1dYPN1dLRIknp6RgKbc8H09HSgWa1NTExofHw87Bhla2hoiFTeoJ+71trb24edczuWbHDOlb1IclFaBgYGXJBaW/3FQtBZrXV3d4f+33clS9TyRu35IGnIlegM7sEAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwQAwQ8EAMEPBADBDwZQpm5XyeWlwUEok/PVKls36OdesiUZeVKd1YQeIgmxWSqel2Vl/fXTUX5ekVCq8XMtZyFso+OuVnhfVi4IpQ2fn4ot1QaEg7dsn9fYGc4zJyRbV1wczVz6/WIYLCgX/+6BgsJq4RCrD2Fjp8XNfxJViuVzLfR+AFc5gytDY6F9mnKupScrlgjlGLjeitra2QOZKJErnbWwMZHqgbJzBlKGrS4rFisdiMX+8EkUtL6oXBVOGVErKZPwzFs/zP2YylXs/YyFvTY2/Xul5Ub24RCpTKhWtF2gqtXgDOqjLOGClOIMBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYGZFBZNMJuWci8wCO1F7LkQtb7XwLvTNeJ6XlpSWpHg8nuzv71+NXIGYnp5WXV1d2DHKYpG1o6NFktTTMxLovFK0HluJvNba29uHnXM7lmxY4f8FXJQMDAyEHaFsFllbW/3FQpQeW+fIa03SkCvRGdyDAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoIBYIaCAWCGggFghoKpUtmslM9Lg4NSIuGvV7Js1s+5Zg15q8m6sAMgeNmslE5Ls7P++uiovy5JqVR4uZazkLdQ8NfJWz0omCrU2bn45F9QKEj79km9vcEcY3KyRfX1wcyVzy+W4YIo5u3spGDOxSVSFRobKz1+7ouiUiyXK2p5l3vcL2ecwVShxkb/tP1cTU1SLhfMMXK5EbW1tQUyVyJRHXkbGwOZvqpwBlOFurqkWKx4LBbzxysReasXBVOFUikpk/HPADzP/5jJVO79gajmranx1ys9b5i4RKpSqVS0nvBRzLtwAzqoy7hqxBkMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADOec+78O3heWlJakuLxeLK/v381cgVienpadXV1YccoS5SySuSVpI6OFklST89IoPNK0Xt829vbh51zO5ZscM6VvSSTSRclAwMDYUcoW5SyOkde55xrbfUXC1F7fCUNuRKdwSUSUMm2bPH/UHGlLlu2nDc+BQNUsomJsBOc3wXyUTAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDAAzFAwAMxQMADMUDDARchmpXxeGhyUEgl/vZJltVsJHdcazSmh48pq96ocd92qHAWoItmslE5Ls7P++uiovy5JqVR4uZaT1W6l1auCaiVJo0oorV5JUkp9psemYIAV6uyUCoXisUJB2rdP6u0N5hiTky2qr5ekgUueK6+dmtWVRWMF1apT3zcvGC6RgBUaGys9vnBGU2lmVVNyfEyN5sfmDAZYocZG/7LoXE1NUi4XzDFyuRG1tbVJXvslz5XQcY0qsWS8Ucs0ZYA4gwFWqKtLisWKx2Ixf7wSdelBxTRTNBbTjLr0oPmxKRhghVIpKZPxz1g8z/+YyYR4g3dqSrr++mU3p9SnjO5Vk17T1JTTLdePK6N7l7//0toqvf56ING4RAIuQioVUqEcPy7F49Lc3OLYtm3SG2+c98tS6vMLZYP0gnHED+MMBoiaXbukDRsWlwuUS5goGCDqnJM++lH/8yeekB59VDp8WHr3Xf/dgDfcUHrf226Tjhzx9xsfl7797eJ59+/3/2G1Eyekr3zloqJRMEC1ufNO6ZFHpE2bpN//fvm7zz/8ofS1r0lXXSVt3y49//ziti1bpI0bpa1b/Tf4PPaY3n9jzopQMEDU/PSn0ttv+8szzyzd/swz0m9/69+nyWallpbS85w5I33iE/5l1uSk9LvfFW/73veks2eln/1Mmp6WmptXHJWCAaLm85/3z042bZK+8IWl20+eXPy8UJDq6krP86UvSbff7r+pJ5eTdu5c3HbqVPGN5PPNcx4UDHC5Ghryy2rzZv+s6ODBwA9BwQCXo/XrpT17/PsvZ8/6N3rn5wM/DO+DAS5Xd93l/8Rp7Vrp2DGTN/ZQMECUlHrHructfr53b/G2wUHpuutK73vbbaWPce7XLHfcMnCJBMAMBQPADAUDwAwFA8AMBQPADAUDwAwFA1SyeDzsBOd3gXy8DwaoZB/+vaIIumDBeJ6XlvT+v/qiac/zjtlGCtQ1kt4KO0SZopRVIq+1qOUt+avWnnNutYOsGs/zhpxzO8LOUY4oZZXIa61a8nIPBoAZCgaAmWovmEzYAVYgSlkl8lqrirxVfQ8GQLiq/QwGQIgoGABmKBgAZigYAGYoGABm/h8uLvzF7SD4NwAAAABJRU5ErkJggg==\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","def make_maze_pic(maze):\n","  maze_pic=[]\n","  for i in range(len(maze)):\n","    row = []\n","    for j in range(len(maze[i])):\n","      if maze[i][j] == 'S':\n","        row.append(0)\n","      if maze[i][j] == 'F':\n","        row.append(0)\n","      if maze[i][j] == 'H':\n","        row.append(1)\n","      if maze[i][j] == 'G':\n","        row.append(0)\n","    maze_pic.append(row)\n","  maze_pic = np.array(maze_pic)\n","  return maze_pic\n","  \n","\n","#Make maze fit to plot\n","maze_pic = make_maze_pic(random_map)\n","nrows, ncols = maze_pic.shape\n","\n","#Arrays of picture elements\n","rw = np.remainder(states,nrows)\n","cl = np.floor_divide(states,nrows)\n","if wn == 1:\n","  rw = np.append(rw, [nrows-1])\n","  cl = np.append(cl,[ncols-1])\n","\n","#Picture plotting\n","fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n","ax1.clear()\n","ax1.set_xticks(np.arange(0.5, nrows, step=1))\n","ax1.set_xticklabels([])\n","ax1.set_yticks(np.arange(0.5, ncols, step=1))\n","ax1.set_yticklabels([])\n","ax1.grid(True)\n","ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n","ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n","ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n","ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n","ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n","ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n","ax1.imshow(maze_pic, cmap=\"binary\")"]},{"cell_type":"markdown","metadata":{"id":"5m14YFyrI6M0"},"source":["# Задача 3"]},{"cell_type":"markdown","metadata":{"id":"lb3BvDuBxTO0"},"source":["Дублируйте полученный блокнот и используйте вместо алгоритма Q-обучения алгоритм SARSA. Обратите внимание на то, что в задании требуется изменить количество игр. То есть `total_games = 40000`. Запускать блоки следует последвательно с самого начала (из-за `random_seed`). Отдельно обращаем ваше внимание на то, что при изменении алгоритма с Q-обучения на SARSA модификации подлежит как процесс обучения, так и функция `learn()`. Кроме того, у функции `learn()` должен появиться дополнительный аргумент (следующее действие). Ниже приведен фрагмент кода с пояснениями, как именно нужно модифицировать алгоритм.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSVpTwTAJO7d"},"outputs":[],"source":["from tqdm import tqdm\n","np.random.seed(random_seed)\n","total_games = 40000\n","max_steps = 100\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","#Main cycle\n","for game in tqdm(range(total_games)):\n","    state = env.reset()\n","    t = 0\n","    action = #Выбор действия в самом начале каждой игры\n","    while t \u003c max_steps:\n","              \n","        t += 1\n","\n","        state2, reward, done, info = env.step(action)\n","\n","        action2 =  #выбор действия как для следующего шага игры, так и для обновления ценности совершенного действия\n","\n","        if t == max_steps:\n","          done = True  \n","\n","        learn(state, state2, reward, action, action2, done) # action2 также передается в функцию обучения\n","\n","        state = state2\n","\n","        action = action2\n","\n","        if done:\n","          break"]},{"cell_type":"markdown","metadata":{"id":"RB_PX2vYIY0-"},"source":[". В результате обучения должны получиться следующие ответы:\n","\n","\n","\n","*   Количество побед в серии из 40 000 игр:  32328\n","*   Пять побед подряд впервые было одержано в игре  894"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}